import os
cache_path = "Z:\\Development Projects\\huggingface"  # Modify as needed
os.environ["TRANSFORMERS_CACHE"] = cache_path
os.environ["HF_HUB_CACHE"] = cache_path
os.environ["HF_HOME"] = cache_path
import sys
import numpy as np
from PIL import Image
import torch
from diffusers import (
    ControlNetModel,
    StableDiffusionXLControlNetPipeline
)
from huggingface_hub import login, hf_hub_download
from controlnet_aux import AnylineDetector  # used for edge detection
from pydantic import BaseModel

# # Replace with your actual token if needed
# login(token="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")

# ------------------------------------------------------------------------------
# Edge Detection

def generate_edges(input_image: Image.Image) -> Image.Image:
    """
    Given an input PIL image, returns an edge map generated by the AnylineDetector.
    If the output is a NumPy array, it is converted to a PIL Image.
    """
    img_processor = AnylineDetector.from_pretrained(
        "TheMistoAI/MistoLine",
        filename="MTEED.pth",
        subfolder="Anyline"
    )
    edge_result = img_processor(input_image)
    if isinstance(edge_result, np.ndarray):
        edge_result = Image.fromarray(edge_result)
    return edge_result

# ------------------------------------------------------------------------------
# Configuration Model

class Item(BaseModel):
    prompt: str = "a stunning view of a cluster of modular pavilions nestled within the lush Brazilian jungle, the roof is built using woven bamboo elements, surrounded by majestic mountains rising in the background and a serene river flowing in the foreground, the trees are way taller than the pavilions, earthy tones that blend harmoniously with the yellowish greens of the surrounding jungle, volumetric sunlight goes across the jungle, creating fascinating light rays, 4k, high resolution, realistic render, architectural visualization"
    negative_prompt: str = "ugly, low quality"
    guidance_scale: float = 5.0
    control_strength: float = 0.5  
    input_image: str = "assets/readme_images/test.jpg" 
    seed: int = 0
    num_inference_steps: int = 12

# ------------------------------------------------------------------------------
# Main Execution

def main():
    # Create a configuration instance
    config = Item()

    # Load the input image
    try:
        input_img = Image.open(config.input_image).convert("RGB")
    except Exception as exc:
        print(f"Error loading image from '{config.input_image}': {exc}")

    edge_map = generate_edges(input_img)
    edge_map.save("edge_map.png")
    edge_map.show()

    # define the seed value for the generator
    generator = torch.Generator("cuda").manual_seed(config.seed)

    # Model IDs and LoRA Weights
    # Ensure that both model dimensions are matching
    base_model_id = "SG161222/RealVisXL_V4.0"  
    controlnet_model_id = "diffusers/controlnet-canny-sdxl-1.0"
    # Hyper Lora model 
    repo_name = "ByteDance/Hyper-SD"
    ckpt_name = "Hyper-SDXL-8steps-lora.safetensors"

    # Load ControlNet Model
    print("Loading ControlNet model")
    controlnet = ControlNetModel.from_pretrained(
        controlnet_model_id,
        torch_dtype=torch.float16,
        cache_dir=cache_path
    )
    # Load the SDXL ControlNet Pipeline

    print("Loading diffusion pipeline")
    pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
        base_model_id,
        controlnet=controlnet,
        torch_dtype=torch.float16,
        cache_dir=cache_path
    )
    # Load and Fuse LoRA Weights
    print("Loading and fusing LoRA weights")
    lora_path = hf_hub_download(repo_name, ckpt_name)
    pipe.load_lora_weights(lora_path)
    pipe.fuse_lora(lora_scale=0.125)

    # Device Setup
    pipe.to("cuda")
    pipe.enable_model_cpu_offload()
    # pipe.enable_xformers_memory_efficient_attention()

    # Generate Image with ControlNet

    print("Generating image with ControlNet conditioning")

    result = pipe(
        prompt=config.prompt,
        negative_prompt=config.negative_prompt,
        guidance_scale=config.guidance_scale,
        controlnet_conditioning_scale=config.control_strength,
        generator=generator,
        num_inference_steps=config.num_inference_steps,
        image=edge_map
    )

    generated_image = result.images[0]
    output_path = "assets/output/sdxl-2-2-3-hyper.png"
    generated_image.save(output_path)

    print(f"Image generated and saved to {output_path}")

if __name__ == "__main__":
    main()